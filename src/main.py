import sys
import logging
from logging.handlers import RotatingFileHandler
from pathlib import Path
from typing import Dict, Any, Optional, List

from .cli.arguments import create_parser, validate_args
from .pdf.extractor import PDFExtractor
from .pdf.images import ImageExtractor
from .llm.summarize import Summarizer
from .utils.files import format_bytes, ensure_directory


def setup_logging(verbose: bool = False, quiet: bool = False, log_to_file: bool = True, run_id: Optional[str] = None) -> None:
    """Configura o sistema de logging da aplica√ß√£o com suporte a arquivo e console.
    
    Args:
        verbose: Se True, exibe logs DEBUG
        quiet: Se True, exibe apenas logs ERROR
        log_to_file: Se True, salva logs em arquivo com rota√ß√£o
        run_id: Identificador √∫nico da execu√ß√£o para agrupar logs
    """
    if quiet:
        log_level = logging.ERROR
    elif verbose:
        log_level = logging.DEBUG
    else:
        log_level = logging.INFO
    
    if run_id:
        log_format = f'%(asctime)s - [{run_id}] - %(name)s - %(levelname)s - %(message)s'
    else:
        log_format = '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    
    date_format = '%Y-%m-%d %H:%M:%S'
    
    handlers = []
    
    console_handler = logging.StreamHandler()
    console_handler.setLevel(log_level)
    console_handler.setFormatter(logging.Formatter(log_format, date_format))
    handlers.append(console_handler)
    
    if log_to_file:
        try:
            log_dir = ensure_directory('logs')
            log_file = log_dir / 'pdf_analyzer.log'
            
            file_handler = RotatingFileHandler(
                log_file,
                maxBytes=5 * 1024 * 1024,
                backupCount=3,
                encoding='utf-8'
            )
            file_handler.setLevel(logging.DEBUG)
            file_handler.setFormatter(logging.Formatter(log_format, date_format))
            handlers.append(file_handler)
        except Exception as error:
            print(f"[AVISO] N√£o foi poss√≠vel configurar log em arquivo: {error}")
    
    logging.basicConfig(
        level=log_level,
        format=log_format,
        datefmt=date_format,
        handlers=handlers
    )


def print_analysis_results(pdf_analysis: Dict[str, Any]) -> None:
    """
    Exibe os resultados da an√°lise do PDF na sa√≠da padr√£o.
    
    Args:
        pdf_analysis: Dicion√°rio contendo os resultados da an√°lise
    """
    SEPARATOR = "=" * 70
    
    print(f"\n{SEPARATOR}")
    print("AN√ÅLISE DO PDF")
    print(SEPARATOR)
    print(f"\nArquivo: {pdf_analysis['file_name']}")
    print(f"Caminho: {pdf_analysis['file_path']}")
    print(f"\nüìÑ N√∫mero de p√°ginas: {pdf_analysis['page_count']}")
    print(f"üì¶ Tamanho do arquivo: {format_bytes(pdf_analysis['file_size_bytes'])} ({pdf_analysis['file_size_bytes']:,} bytes)")
    print(f"üìù Total de palavras: {pdf_analysis['word_count']:,}")
    print(f"üìö Tamanho do vocabul√°rio: {pdf_analysis['vocabulary_size']:,} palavras distintas")
    
    if 'avg_words_per_page' in pdf_analysis:
        print(f"üìä M√©dia de palavras por p√°gina: {pdf_analysis['avg_words_per_page']:.2f}")
    if 'lexical_diversity' in pdf_analysis:
        print(f"üìà Diversidade lexical: {pdf_analysis['lexical_diversity']:.2f}%")
    
    print(f"\nüî§ 10 palavras mais comuns (sem stopwords):")
    for position, (word, frequency) in enumerate(pdf_analysis['most_common_words'], start=1):
        print(f"   {position:2}. {word:<20} ({frequency:,} ocorr√™ncias)")
    
    if 'titles' in pdf_analysis and pdf_analysis['titles']:
        print(f"\nüìë T√≠tulos detectados ({len(pdf_analysis['titles'])}):")
        for title in pdf_analysis['titles'][:5]:
            print(f"   - {title}")
    
    if 'sections' in pdf_analysis and pdf_analysis['sections']:
        print(f"\nüìã Se√ß√µes detectadas ({len(pdf_analysis['sections'])}):")
        for section in pdf_analysis['sections'][:5]:
            print(f"   {section['number']} {section['title']}")
    
    if 'keywords' in pdf_analysis and pdf_analysis['keywords']:
        print(f"\nüîë Palavras-chave principais:")
        keywords_list = [word for word, _ in pdf_analysis['keywords'][:10]]
        print(f"   {', '.join(keywords_list)}")
    
    print()


def print_image_results(extracted_images: List[str], output_directory: str) -> None:
    """
    Exibe os resultados da extra√ß√£o de imagens.
    
    Args:
        extracted_images: Lista de caminhos das imagens extra√≠das
        output_directory: Diret√≥rio onde as imagens foram salvas
    """
    SEPARATOR = "=" * 70
    MAX_IMAGES_TO_SHOW = 5
    
    print(SEPARATOR)
    print("EXTRA√á√ÉO DE IMAGENS")
    print(SEPARATOR)
    print(f"\nüñºÔ∏è  Total de imagens extra√≠das: {len(extracted_images)}")
    
    if extracted_images:
        print(f"üìÅ Diret√≥rio de sa√≠da: {output_directory}")
        print(f"\nPrimeiras imagens:")
        
        for image_path in extracted_images[:MAX_IMAGES_TO_SHOW]:
            print(f"   - {Path(image_path).name}")
        
        remaining_images = len(extracted_images) - MAX_IMAGES_TO_SHOW
        if remaining_images > 0:
            print(f"   ... e mais {remaining_images} imagens")
    
    print()


def print_summary(summary_text: str) -> None:
    """
    Exibe o resumo gerado pelo modelo LLM.
    
    Args:
        summary_text: Texto do resumo gerado
    """
    SEPARATOR = "=" * 70
    
    print(SEPARATOR)
    print("RESUMO DO DOCUMENTO (gerado por LLM)")
    print(SEPARATOR)
    print(f"\n{summary_text}\n")


def generate_markdown_report(
    pdf_analysis: Dict[str, Any],
    extracted_images: List[str],
    summary_text: Optional[str],
    report_output_path: str
) -> None:
    """
    Gera relat√≥rio completo em formato Markdown.
    
    Args:
        pdf_analysis: Resultados da an√°lise do PDF
        extracted_images: Lista de imagens extra√≠das
        summary_text: Resumo gerado (ou None se desabilitado)
        report_output_path: Caminho onde salvar o relat√≥rio
    """
    logger = logging.getLogger(__name__)
    logger.info(f"Gerando relat√≥rio Markdown: {report_output_path}")
    
    from datetime import datetime
    
    with open(report_output_path, 'w', encoding='utf-8') as report_file:
        report_file.write("# üìä Relat√≥rio Completo de An√°lise de PDF\n\n")
        report_file.write(f"**Gerado em**: {datetime.now().strftime('%d/%m/%Y √†s %H:%M:%S')}\n\n")
        report_file.write("---\n\n")
        
        report_file.write("## üìÑ Informa√ß√µes do Documento\n\n")
        report_file.write(f"- **Arquivo**: `{pdf_analysis['file_name']}`\n")
        report_file.write(f"- **Caminho**: `{pdf_analysis['file_path']}`\n")
        report_file.write(f"- **N√∫mero de p√°ginas**: {pdf_analysis['page_count']}\n")
        report_file.write(f"- **Tamanho**: {format_bytes(pdf_analysis['file_size_bytes'])} ({pdf_analysis['file_size_bytes']:,} bytes)\n")
        report_file.write(f"- **Total de palavras**: {pdf_analysis['word_count']:,}\n")
        report_file.write(f"- **Vocabul√°rio**: {pdf_analysis['vocabulary_size']:,} palavras distintas\n\n")
        
        report_file.write("## üî§ Palavras Mais Comuns\n\n")
        report_file.write("| # | Palavra | Frequ√™ncia |\n")
        report_file.write("|---|---------|------------|\n")
        for position, (word, frequency) in enumerate(pdf_analysis['most_common_words'], start=1):
            report_file.write(f"| {position} | {word} | {frequency:,} |\n")
        report_file.write("\n")
        
        if 'titles' in pdf_analysis and pdf_analysis['titles']:
            report_file.write("## üìë T√≠tulos Detectados\n\n")
            for title in pdf_analysis['titles']:
                report_file.write(f"- {title}\n")
            report_file.write("\n")
        
        if 'sections' in pdf_analysis and pdf_analysis['sections']:
            report_file.write("## üìã Se√ß√µes Identificadas\n\n")
            for section in pdf_analysis['sections']:
                report_file.write(f"- **{section['number']}** {section['title']}\n")
            report_file.write("\n")
        
        if 'keywords' in pdf_analysis and pdf_analysis['keywords']:
            report_file.write("## üîë Palavras-Chave Principais\n\n")
            keywords_text = ', '.join([word for word, _ in pdf_analysis['keywords']])
            report_file.write(f"{keywords_text}\n\n")
        
        report_file.write("## üñºÔ∏è Imagens Extra√≠das\n\n")
        report_file.write(f"**Total**: {len(extracted_images)} imagens\n\n")
        if extracted_images:
            report_file.write("### Lista de Imagens\n\n")
            for image_path in extracted_images:
                report_file.write(f"- `{Path(image_path).name}`\n")
            report_file.write("\n")
        
        if summary_text:
            report_file.write("## üìù Resumo Gerado por LLM\n\n")
            report_file.write(f"> {summary_text}\n\n")
        else:
            report_file.write("## üìù Resumo Gerado por LLM\n\n")
            report_file.write("*Resumo n√£o gerado (use --summarize para ativar)*\n\n")
        
        report_file.write("---\n\n")
        report_file.write("## üìà Estat√≠sticas Consolidadas\n\n")
        report_file.write(f"- Total de p√°ginas analisadas: **{pdf_analysis['page_count']}**\n")
        report_file.write(f"- Palavras √∫nicas no vocabul√°rio: **{pdf_analysis['vocabulary_size']:,}**\n")
        report_file.write(f"- Taxa de diversidade lexical: **{(pdf_analysis['vocabulary_size'] / max(pdf_analysis['word_count'], 1) * 100):.2f}%**\n")
        
        if 'titles' in pdf_analysis:
            report_file.write(f"- T√≠tulos identificados: **{len(pdf_analysis['titles'])}**\n")
        if 'sections' in pdf_analysis:
            report_file.write(f"- Se√ß√µes estruturadas: **{len(pdf_analysis['sections'])}**\n")
        
        report_file.write(f"- Imagens extra√≠das: **{len(extracted_images)}**\n")
        report_file.write(f"- Resumo LLM: **{'‚úì Gerado' if summary_text else '‚úó N√£o gerado'}**\n\n")
        
        report_file.write("---\n\n")
        report_file.write("*Relat√≥rio gerado automaticamente pela ferramenta CLI de An√°lise de PDF com Sumariza√ß√£o por LLM*\n")
    
    logger.info(f"Relat√≥rio salvo em: {report_output_path}")


def _run_pdf_analysis(args: Any, logger: logging.Logger) -> Dict[str, Any]:
    """Executa an√°lise do PDF com par√¢metros configur√°veis.
    
    Args:
        args: Argumentos parseados da linha de comando
        logger: Logger configurado
    
    Returns:
        Dicion√°rio com resultados da an√°lise
    """
    logger.info("Iniciando an√°lise do PDF...")
    
    with PDFExtractor(args.pdf_file) as extractor:
        if extractor.get_page_count() == 0:
            raise ValueError("[ERRO] PDF est√° vazio (0 p√°ginas)")
        
        max_pages = args.max_pages if args.max_pages else None
        if max_pages and extractor.get_page_count() > max_pages:
            logger.info(f"Limitando an√°lise aos primeiros {max_pages} p√°ginas")
        
        analysis = extractor.analyze(
            word_mode=args.word_mode,
            keep_numbers=args.keep_numbers,
            top_n_words=args.top_n_words,
            analyze_structure=not args.no_structure
        )
    
    return analysis


def _run_image_extraction(args: Any, logger: logging.Logger) -> List[str]:
    """Executa extra√ß√£o de imagens do PDF.
    
    Args:
        args: Argumentos parseados da linha de comando
        logger: Logger configurado
    
    Returns:
        Lista de caminhos das imagens extra√≠das
    """
    logger.info("Iniciando extra√ß√£o de imagens...")
    
    with ImageExtractor(args.pdf_file) as img_extractor:
        pdf_name = Path(args.pdf_file).stem
        output_dir = args.output_dir or f"outputs/images/{pdf_name}"
        image_paths = img_extractor.extract_images(
            output_dir,
            min_size=args.min_image_size
        )
    
    return image_paths


def _run_summarization(args: Any, analysis: Dict[str, Any], logger: logging.Logger) -> Optional[str]:
    """Executa gera√ß√£o de resumo com LLM.
    
    Args:
        args: Argumentos parseados da linha de comando
        analysis: Resultados da an√°lise do PDF
        logger: Logger configurado
    
    Returns:
        Texto do resumo ou None
    """
    logger.info("Iniciando gera√ß√£o de resumo com LLM...")
    print("="*70)
    print("Gerando resumo com modelo de linguagem...")
    print("(Isso pode levar alguns minutos na primeira execu√ß√£o)")
    print("="*70 + "\n")
    
    summarizer = Summarizer(model_name=args.model)
    summary = summarizer.summarize(
        analysis['full_text'],
        deterministic=args.deterministic
    )
    summarizer.cleanup()
    
    return summary


def _generate_report_with_metadata(
    args: Any,
    analysis: Dict[str, Any],
    image_paths: List[str],
    summary: Optional[str],
    run_id: str,
    duration: float
) -> str:
    """Gera relat√≥rio com metadados de execu√ß√£o.
    
    Args:
        args: Argumentos da linha de comando
        analysis: Resultados da an√°lise
        image_paths: Imagens extra√≠das
        summary: Resumo gerado
        run_id: ID da execu√ß√£o
        duration: Dura√ß√£o total em segundos
    
    Returns:
        Caminho do relat√≥rio gerado
    """
    from datetime import datetime
    import sys
    
    pdf_name = Path(args.pdf_file).stem
    report_path = args.report or f"outputs/relatorio_{pdf_name}.md"
    
    logger = logging.getLogger(__name__)
    logger.info(f"Gerando relat√≥rio: {report_path}")
    
    with open(report_path, 'w', encoding='utf-8') as f:
        f.write("# üìä Relat√≥rio Completo de An√°lise de PDF\n\n")
        f.write(f"**Gerado em**: {datetime.now().strftime('%d/%m/%Y √†s %H:%M:%S')}\n")
        f.write(f"**Run ID**: `{run_id}`\n")
        f.write(f"**Dura√ß√£o**: {duration:.2f}s\n\n")
        f.write("## üîß Configura√ß√£o da Execu√ß√£o\n\n")
        f.write(f"**Comando**: `{' '.join(sys.argv)}`\n\n")
        f.write(f"- Modo de contagem: `{args.word_mode}`\n")
        f.write(f"- N√∫meros contados: `{'Sim' if args.keep_numbers else 'N√£o'}`\n")
        f.write(f"- Top N palavras: `{args.top_n_words}`\n")
        f.write(f"- An√°lise de estrutura: `{'N√£o' if args.no_structure else 'Sim'}`\n")
        if not args.no_summary:
            f.write(f"- Modelo LLM: `{args.model}`\n")
            f.write(f"- Modo determin√≠stico: `{'Sim' if args.deterministic else 'N√£o'}`\n")
        f.write("\n---\n\n")
        
        f.write("## üìÑ Informa√ß√µes do Documento\n\n")
        f.write(f"- **Arquivo**: `{analysis['file_name']}`\n")
        f.write(f"- **Caminho**: `{analysis['file_path']}`\n")
        f.write(f"- **N√∫mero de p√°ginas**: {analysis['page_count']}\n")
        f.write(f"- **Tamanho**: {format_bytes(analysis['file_size_bytes'])} ({analysis['file_size_bytes']:,} bytes)\n")
        f.write(f"- **Total de palavras**: {analysis['word_count']:,}\n")
        f.write(f"- **Vocabul√°rio**: {analysis['vocabulary_size']:,} palavras distintas\n")
        
        if 'avg_words_per_page' in analysis:
            f.write(f"- **M√©dia palavras/p√°gina**: {analysis['avg_words_per_page']:.2f}\n")
        if 'lexical_diversity' in analysis:
            f.write(f"- **Diversidade lexical**: {analysis['lexical_diversity']:.2f}%\n")
        f.write("\n")
        
        f.write("## üî§ Palavras Mais Comuns\n\n")
        f.write("| # | Palavra | Frequ√™ncia |\n")
        f.write("|---|---------|------------|\n")
        for i, (word, freq) in enumerate(analysis['most_common_words'], 1):
            f.write(f"| {i} | {word} | {freq:,} |\n")
        f.write("\n")
        
        if analysis.get('titles'):
            f.write("## üìë T√≠tulos Detectados\n\n")
            for title in analysis['titles']:
                f.write(f"- {title}\n")
            f.write("\n")
        
        if analysis.get('sections'):
            f.write("## üìã Se√ß√µes Identificadas\n\n")
            for section in analysis['sections']:
                f.write(f"- **{section['number']}** {section['title']}\n")
            f.write("\n")
        
        if analysis.get('keywords'):
            f.write("## üîë Palavras-Chave Principais\n\n")
            keywords_text = ', '.join([word for word, _ in analysis['keywords']])
            f.write(f"{keywords_text}\n\n")
        
        f.write("## üñºÔ∏è Imagens Extra√≠das\n\n")
        f.write(f"**Total**: {len(image_paths)} imagens\n\n")
        if image_paths:
            f.write("### Lista de Imagens\n\n")
            for img_path in image_paths:
                f.write(f"- `{Path(img_path).name}`\n")
            f.write("\n")
        
        if summary:
            f.write("## üìù Resumo Gerado por LLM\n\n")
            f.write(f"> {summary}\n\n")
        
        f.write("---\n\n")
        f.write("*Relat√≥rio gerado automaticamente pela ferramenta CLI de An√°lise de PDF*\n")
    
    return report_path


def main() -> int:
    """Fun√ß√£o principal do programa."""
    import time
    from datetime import datetime
    
    parser = create_parser()
    args = parser.parse_args()
    
    if not validate_args(args):
        return 1
    
    # Gera ID √∫nico para esta execu√ß√£o
    run_id = datetime.now().strftime("%Y%m%d_%H%M%S")
    start_time = time.time()
    
    setup_logging(verbose=args.verbose, quiet=args.quiet, log_to_file=args.log, run_id=run_id)
    logger = logging.getLogger(__name__)
    
    logger.info(f"[Run ID: {run_id}] Iniciando processamento do PDF")
    
    try:
        # 1. An√°lise do PDF
        analysis = _run_pdf_analysis(args, logger)
        print_analysis_results(analysis)
        
        # 2. Extra√ß√£o de imagens
        image_paths = []
        if not args.no_images:
            image_paths = _run_image_extraction(args, logger)
            output_dir = args.output_dir or f"outputs/images/{Path(args.pdf_file).stem}"
            print_image_results(image_paths, output_dir)
        
        # 3. Gera√ß√£o de resumo com LLM
        summary = None
        if not args.no_summary:
            summary = _run_summarization(args, analysis, logger)
            if summary:
                print_summary(summary)
        
        # 4. Gera√ß√£o de relat√≥rio final
        duration = time.time() - start_time
        report_path = args.report
        if not report_path:
            pdf_name = Path(args.pdf_file).stem
            report_path = f"outputs/relatorio_{pdf_name}.md"
        
        _generate_report_with_metadata(args, analysis, image_paths, summary, run_id, duration)
        print(f"üìã Relat√≥rio completo salvo em: {report_path}\n")
        
        print("="*70)
        print(f"‚úÖ Processamento conclu√≠do em {duration:.2f}s!")
        print("="*70 + "\n")
        
        logger.info(f"[Run ID: {run_id}] Processamento conclu√≠do com sucesso em {duration:.2f}s")
        return 0
    
    except FileNotFoundError as e:
        logger.error(f"[ERRO] Arquivo n√£o encontrado: {e}")
        print(f"\n‚ùå [ERRO] Arquivo n√£o encontrado: {e}\n")
        return 1
    
    except ValueError as e:
        logger.error(f"[ERRO] Valida√ß√£o: {e}")
        print(f"\n‚ùå {e}\n")
        return 1
    
    except Exception as e:
        logger.error(f"[ERRO] Erro durante o processamento: {e}", exc_info=True)
        print(f"\n‚ùå [ERRO] Erro inesperado: {e}\n")
        return 1


if __name__ == '__main__':
    sys.exit(main())
